<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chen Tessler</title>
    <link>https://tesslerc.github.io/</link>
    <description>Recent content on Chen Tessler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 06 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tesslerc.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Reinforcement Learning Works - Now What?</title>
      <link>https://tesslerc.github.io/posts/drl_works_now_what/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/posts/drl_works_now_what/</guid>
      <description>It&amp;rsquo;s safe to assume that deep reinforcement learning does indeed work. This is backed by recent trends which have achieved tremendous feats. An important question is — now what? In this post I question certain trends in deep RL research and propose some insights and solutions.</description>
    </item>
    
    <item>
      <title>Maximizing the Total Reward via Reward Tweaking</title>
      <link>https://tesslerc.github.io/portfolio/reward_tweaking/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/reward_tweaking/</guid>
      <description>In many practical applications, we train the agent on the $\gamma$-discounted task and evaluate it on the total reward. The discrepancy between training and evaluation may lead to sub-optimal solutions. Reward tweaking learns an alternative surrogate reward, aimed to guide the agent towards better behavior on the evaluation metric.</description>
    </item>
    
    <item>
      <title>Inverse Reinforcement Learning in Contextual MDPs</title>
      <link>https://tesslerc.github.io/portfolio/coirl/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/coirl/</guid>
      <description>Real-world sequential decision problems often share two important properties &amp;ndash; the reward function is often unknown, yet expert demonstrations can be acquired, and there often exists a static parameter, also known as the context, which determines certain aspects of the problem. In this work we formalize the Contextual Inverse Reinforcement Learning framework, propose several algorithms and analyze them both theoretically and empirically.</description>
    </item>
    
    <item>
      <title>Distributional Policy Optimization: An Alternative Approach for Continuous Control</title>
      <link>https://tesslerc.github.io/portfolio/dpo/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/dpo/</guid>
      <description>We propose a method for learning distributional policies, policies which are not limited to parametric distribution functions (e.g., Gaussian and Delta). This approach overcomes sub-optimal local extremum in continuous control regimes.</description>
    </item>
    
    <item>
      <title>Action Robust Reinforcement Learning and Applications in Continuous Control</title>
      <link>https://tesslerc.github.io/portfolio/action_robust/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/action_robust/</guid>
      <description>Action Robust is a special case of robustness, in which the agent is robust to uncertainty in the performed action. We show (theoretically) that this form of robustness has efficient solutions and (empirically) results in policies which are robust to common uncertainties in robotic domains.</description>
    </item>
    
    <item>
      <title>Reward Constrained Policy Optimization</title>
      <link>https://tesslerc.github.io/portfolio/rcpo/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/rcpo/</guid>
      <description>Learning a policy which adheres to behavioral constraints is an important task. Our algorithm, RCPO, enables the satisfaction of not only discounted constraints but also average and probabilistic, in an efficient manner.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://tesslerc.github.io/about/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/about/</guid>
      <description>I&amp;rsquo;m currently pursuing my P.hD in Reinforcement Learning at the Technion Institute of Technology, Israel, under the supervision of Prof. Shie Mannor.
Reinforcement Learning (RL) is a learning paradigm that highly resembles how we, as humans, learn. The agent, i.e., the learning algorithm, learns through interaction with the environment. By observing the current state of the system, it decides what action to take, after which the environment transitions to a new state and produces the agent with a reward.</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>https://tesslerc.github.io/cv/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/cv/</guid>
      <description>Higher education 2017-Today
P.hD. Candidate - Electrical Engineering
Technion Institute of Technology, Israel
 Specialization in Reinforcement Learning, supervised by Prof. Shie Mannor  2017-2019
M.Sc. - Electrical Engineering
Technion Institute of Technology, Israel
 Specialization in Reinforcement Learning, supervised by Prof. Shie Mannor  2014–2017
B.Sc. (cum laude) - Electrical Engineering
Technion Institute of Technology, Israel
 Research oriented EE faculty excellence program (EMET) Specialized in Machine Learning  2009–2012</description>
    </item>
    
    <item>
      <title>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
      <link>https://tesslerc.github.io/portfolio/hdrln/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/hdrln/</guid>
      <description>We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem.</description>
    </item>
    
  </channel>
</rss>