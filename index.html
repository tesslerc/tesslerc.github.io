<!DOCTYPE html>
<html lang="en">
    
  <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.73.0" />

    

<title>Chen Tessler</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Chen Tessler"/>
<meta name="twitter:description" content=""/>

<meta property="og:title" content="Chen Tessler" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://tesslerc.github.io/" />
<meta property="og:updated_time" content="2021-01-01T00:00:00+00:00" /><meta property="og:site_name" content="Chen Tessler" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.919cd87a8432a9d1f4fcc089bd2b05ae8c035af181b310c7ff59f805fcee805f.css" integrity="sha256-kZzYeoQyqdH0/MCJvSsFrowDWvGBsxDH/1n4BfzugF8=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    <link rel="alternate" type="application/rss+xml" href="https://tesslerc.github.io/index.xml" title="Chen Tessler" />

    

    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://tesslerc.github.io/">Chen Tessler</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://tesslerc.github.io/img/chen_tessler.jpg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
        
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Chen Tessler</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Research</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/cv/">
						<span>Curriculum Vitae</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/tesslerc" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/tesslerc" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/chentessler" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="mailto:chen.tessler@gmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2021 htr3n
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
  <span class="section__title">Recent posts</span>
    <section style="margin-bottom:18rem">
    <hr class="divider">
      <div class="row">
          <div class="col-md-4 col-sm-4 col-xs-12">
            <time class="pull-left hidden-tablet">May 6 2020</time>
          </div>
          </br></br>
          <div class="col-md-4 col-sm-4 col-xs-12" href="/"
               target="_blank" rel="noopener noreferrer">
              <a href="/posts/drl_works_now_what">
                <img class="project__image img-responsive" src="/posts/drl_works_theme_image.png" alt="Deep Reinforcement Learning Works - Now What?">
              </a>
          </div>
          <div class="col-md-8 col-sm-8 col-xs-12">
              <span>
                  <a href="/posts/drl_works_now_what">Deep Reinforcement Learning Works - Now What?</a>
              </span>
              </br>
              <span class="project__summary">
                  It's safe to assume that deep reinforcement learning does indeed work. This is backed by recent trends which have achieved tremendous feats. An important question is â€” now what? In this post I question certain trends in deep RL research and propose some insights and solutions.
              </span>
        </div>
      </div>
    </section>
  <span class="section__title">Accepted Papers</span>
    <hr class="divider">
    <section>
      <div class="portfolio__content">
        
        
          
            
          

        

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://arxiv.org/abs/1905.09710"
                    target="_blank" rel="noopener noreferrer">
                    <a href="https://arxiv.org/abs/1905.09710" target="_blank" rel="noopener noreferrer">
                        <img class="project__image img-responsive" src="portfolio/coirl.png" alt="Inverse Reinforcement Learning in Contextual MDPs">
                    </a>
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://arxiv.org/abs/1905.09710" target="_blank" rel="noopener noreferrer">Inverse Reinforcement Learning in Contextual MDPs</a>
                    </span>
                    
                        </br>
                        <span class="project__authors">
                            Stav Belogolovsky*, Philip Korsunsky*, Shie Mannor*, Chen Tessler* and Tom Zahavy*
                        </span>
                    
                    </br>
                    <span class="project__authors">
                        <b>Published: </b>Springer Machine Learning 2021
                    </span>





                    <span class="project__summary">
                        <p>Real-world sequential decision problems often share two important properties &ndash; the reward function is often unknown, yet expert demonstrations can be acquired, and there often exists a static parameter, also known as the context, which determines certain aspects of the problem.
In this work we formalize the Contextual Inverse Reinforcement Learning framework, propose several algorithms and analyze them both theoretically and empirically.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>

        
         
          
            
          

        
         
          
            
          

        

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control"
                    target="_blank" rel="noopener noreferrer">
                    <a href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control" target="_blank" rel="noopener noreferrer">
                        <img class="project__image img-responsive" src="portfolio/dpo.png" alt="Distributional Policy Optimization: An Alternative Approach for Continuous Control">
                    </a>
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control" target="_blank" rel="noopener noreferrer">Distributional Policy Optimization: An Alternative Approach for Continuous Control</a>
                    </span>
                    
                        </br>
                        <span class="project__authors">
                            Chen Tessler*, Guy Tennenholtz* and Shie Mannor
                        </span>
                    
                    </br>
                    <span class="project__authors">
                        <b>Published: </b>NeurIPS 2019
                    </span>





                    <span class="project__summary">
                        <p>We propose a method for learning distributional policies, policies which are not limited to parametric distribution functions (e.g., Gaussian and Delta). This approach overcomes sub-optimal local extremum in continuous control regimes.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>

        
         
          
            
          

        

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="http://proceedings.mlr.press/v97/tessler19a.html"
                    target="_blank" rel="noopener noreferrer">
                    <a href="http://proceedings.mlr.press/v97/tessler19a.html" target="_blank" rel="noopener noreferrer">
                        <img class="project__image img-responsive" src="portfolio/action_robust.jpeg" alt="Action Robust Reinforcement Learning and Applications in Continuous Control">
                    </a>
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="http://proceedings.mlr.press/v97/tessler19a.html" target="_blank" rel="noopener noreferrer">Action Robust Reinforcement Learning and Applications in Continuous Control</a>
                    </span>
                    
                        </br>
                        <span class="project__authors">
                            Chen Tessler*, Yonathan Efroni* and Shie Mannor
                        </span>
                    
                    </br>
                    <span class="project__authors">
                        <b>Published: </b>ICML 2019
                    </span>





                    <span class="project__summary">
                        <p>Action Robust is a special case of robustness, in which the agent is robust to uncertainty in the performed action. We show (theoretically) that this form of robustness has efficient solutions and (empirically) results in policies which are robust to common uncertainties in robotic domains.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>

        
         
          
            
          

        

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://arxiv.org/pdf/1805.11074"
                    target="_blank" rel="noopener noreferrer">
                    <a href="https://arxiv.org/pdf/1805.11074" target="_blank" rel="noopener noreferrer">
                        <img class="project__image img-responsive" src="portfolio/rcpo.png" alt="Reward Constrained Policy Optimization">
                    </a>
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://arxiv.org/pdf/1805.11074" target="_blank" rel="noopener noreferrer">Reward Constrained Policy Optimization</a>
                    </span>
                    
                        </br>
                        <span class="project__authors">
                            Chen Tessler, Daniel J. Mankowitz and Shie Mannor
                        </span>
                    
                    </br>
                    <span class="project__authors">
                        <b>Published: </b>ICLR 2019
                    </span>





                    <span class="project__summary">
                        <p>Learning a policy which adheres to behavioral constraints is an important task. Our algorithm, RCPO, enables the satisfaction of not only discounted constraints but also average and probabilistic, in an efficient manner.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>

        
         
          
            
          

        

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14630/13950"
                    target="_blank" rel="noopener noreferrer">
                    <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14630/13950" target="_blank" rel="noopener noreferrer">
                        <img class="project__image img-responsive" src="portfolio/hdrln.png" alt="A Deep Hierarchical Approach to Lifelong Learning in Minecraft">
                    </a>
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14630/13950" target="_blank" rel="noopener noreferrer">A Deep Hierarchical Approach to Lifelong Learning in Minecraft</a>
                    </span>
                    
                        </br>
                        <span class="project__authors">
                            Chen Tessler*, Shahar Givony*, Tom Zahavy*, Daniel J. Mankowitz* and Shie Mannor
                        </span>
                    
                    </br>
                    <span class="project__authors">
                        <b>Published: </b>AAAI 2017
                    </span>





                    <span class="project__summary">
                        <p>We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>

        
         
      </div>
    </section>
  


        </div>
        
  
  <script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    </body>
</html>
