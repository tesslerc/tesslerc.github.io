<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on Chen Tessler</title>
    <link>https://tesslerc.github.io/portfolio/</link>
    <description>Recent content in Publications on Chen Tessler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 01 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tesslerc.github.io/portfolio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Maximizing the Total Reward via Reward Tweaking</title>
      <link>https://tesslerc.github.io/portfolio/reward_tweaking/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/reward_tweaking/</guid>
      <description>In many practical applications, we train the agent on the É£-discounted task and evaluate it on the total reward. The discrepancy between training and evaluation may lead to sub-optimal solutions. Reward tweaking learns an alternative surrogate reward, aimed to guide the agent towards better behavior on the evaluation metric.</description>
    </item>
    
    <item>
      <title>Inverse Reinforcement Learning in Contextual MDPs</title>
      <link>https://tesslerc.github.io/portfolio/coirl/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/coirl/</guid>
      <description>We analyze the problem of performing inverse reinforcement learning in contextual MDPs.</description>
    </item>
    
    <item>
      <title>Distributional Policy Optimization: An Alternative Approach for Continuous Control</title>
      <link>https://tesslerc.github.io/portfolio/dpo/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/dpo/</guid>
      <description>We propose a method for learning distributional policies, policies which are not limited to parametric distribution functions (e.g., Gaussian and Delta). This approach overcomes sub-optimal local extremum in continuous control regimes.</description>
    </item>
    
    <item>
      <title>Action Robust Reinforcement Learning and Applications in Continuous Control</title>
      <link>https://tesslerc.github.io/portfolio/action_robust/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/action_robust/</guid>
      <description>Action Robust is a special case of robustness, in which the agent is robust to uncertainty in the performed action. We show (theoretically) that this form of robustness has efficient solutions and (empirically) results in policies which are robust to common uncertainties in robotic domains.</description>
    </item>
    
    <item>
      <title>Reward Constrained Policy Optimization</title>
      <link>https://tesslerc.github.io/portfolio/rcpo/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/rcpo/</guid>
      <description>Learning a policy which adheres to behavioral constraints is an important task. Our algorithm, RCPO, enables the satisfaction of not only discounted constraints but also average and probabilistic, in an efficient manner.</description>
    </item>
    
    <item>
      <title>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
      <link>https://tesslerc.github.io/portfolio/hdrln/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tesslerc.github.io/portfolio/hdrln/</guid>
      <description>We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem.</description>
    </item>
    
  </channel>
</rss>