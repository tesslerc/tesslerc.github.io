<!DOCTYPE html>
<html lang="en">
    
    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.66.0" />

    
    
    

<title>Publications • Chen Tessler</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Publications"/>
<meta name="twitter:description" content=""/>

<meta property="og:title" content="Publications" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://tesslerc.github.io/portfolio/" />
<meta property="og:updated_time" content="2020-02-01T00:00:00+00:00" /><meta property="og:site_name" content="Chen Tessler" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.3081c4981fb69a2783dd36ecfdd0e6ba7a158d4cbfdd290ebce8f78ba0469fc6.css" integrity="sha256-MIHEmB&#43;2mieD3Tbs/dDmunoVjUy/3SkOvOj3i6BGn8Y=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    <link rel="alternate" type="application/rss+xml" href="https://tesslerc.github.io/portfolio/index.xml" title="Chen Tessler" />

    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://tesslerc.github.io/">Chen Tessler</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://tesslerc.github.io/img/chen_tessler.jpg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
        
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Chen Tessler</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Publications</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/tesslerc" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/tesslerc" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/chentessler" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="mailto:chen.tessler@gmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2020 htr3n
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
    <span class="section__title">Publications</span>
<div class="portfolio__content">
    
    <section>
        <div class="portfolio_content">
            <hr class="divider">
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://arxiv.org/abs/2002.03327"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="reward_tweaking.png" alt="Maximizing the Total Reward via Reward Tweaking">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://arxiv.org/abs/2002.03327" target="_blank" rel="noopener noreferrer">Maximizing the Total Reward via Reward Tweaking</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Preprint</b>
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        By learning an alternative surrogate reward we can guide the agent towards better behavior
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>In many practical applications, we train the agent on the ɣ-discounted task and evaluate it on the total reward. The discrepancy between training and evaluation may lead to sub-optimal solutions. Reward tweaking learns an alternative surrogate reward, aimed to guide the agent towards better behavior on the evaluation metric.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://arxiv.org/pdf/1905.09710"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="coirl.png" alt="Inverse Reinforcement Learning in Contextual MDPs">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://arxiv.org/pdf/1905.09710" target="_blank" rel="noopener noreferrer">Inverse Reinforcement Learning in Contextual MDPs</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Preprint</b>
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        In the medical domain, the goal is to provide personalized medicine (contextual MDPs) to each patient, yet the rewards are hard to define (inverse RL)
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>We analyze the problem of performing inverse reinforcement learning in contextual MDPs.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="dpo.png" alt="Distributional Policy Optimization: An Alternative Approach for Continuous Control">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://papers.nips.cc/paper/8416-distributional-policy-optimization-an-alternative-approach-for-continuous-control" target="_blank" rel="noopener noreferrer">Distributional Policy Optimization: An Alternative Approach for Continuous Control</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Published: </b>NeurIPS 2019
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        A generative model for the policy enables the agent to overcome local optima
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>We propose a method for learning distributional policies, policies which are not limited to parametric distribution functions (e.g., Gaussian and Delta). This approach overcomes sub-optimal local extremum in continuous control regimes.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="http://proceedings.mlr.press/v97/tessler19a.html"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="action_robust.jpeg" alt="Action Robust Reinforcement Learning and Applications in Continuous Control">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="http://proceedings.mlr.press/v97/tessler19a.html" target="_blank" rel="noopener noreferrer">Action Robust Reinforcement Learning and Applications in Continuous Control</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Published: </b>ICML 2019
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        A theoretically grounded approach to learning robust policies
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>Action Robust is a special case of robustness, in which the agent is robust to uncertainty in the performed action. We show (theoretically) that this form of robustness has efficient solutions and (empirically) results in policies which are robust to common uncertainties in robotic domains.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://arxiv.org/pdf/1805.11074"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="rcpo.png" alt="Reward Constrained Policy Optimization">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://arxiv.org/pdf/1805.11074" target="_blank" rel="noopener noreferrer">Reward Constrained Policy Optimization</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Published: </b>ICLR 2019
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        Learning to satisfy constraints by converting the constraint into a discounted return
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>Learning a policy which adheres to behavioral constraints is an important task. Our algorithm, RCPO, enables the satisfaction of not only discounted constraints but also average and probabilistic, in an efficient manner.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-12" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14630/13950"
                    target="_blank" rel="noopener noreferrer">
                    <img class="project__image img-responsive" src="hdrln.png" alt="A Deep Hierarchical Approach to Lifelong Learning in Minecraft">
                </div>
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <span class="project__title">
                        <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14630/13950" target="_blank" rel="noopener noreferrer">A Deep Hierarchical Approach to Lifelong Learning in Minecraft</a>
                    </span>
                    </br>
                    
                        <span class="project__summary">
                            <b>Published: </b>AAAI 2017
                        </span>
                    
                    <span class="project__subtitle-small">
                        <sup><i class="fa fa-quote-left" aria-hidden="true"></i></sup>
                        Combining skills with deep RL enables solving complex high dimensional tasks
                        <sup><i class="fa fa-quote-right" aria-hidden="true"></i></sup>
                    </span>
                    <span class="project__summary">
                        <p>We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem.</p>

                    </span>
                </div>
            </div>
            <div class="row-space">&nbsp;</div>
            
        </div>
    </section>
</div>


        </div>
        
    
<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>



    </body>
</html>
